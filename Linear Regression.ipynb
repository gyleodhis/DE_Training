{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Spark\n",
    "### gyleodhis@outlook.com\n",
    "### [@gyleodhis](https://www.twitter.com/gyleodhis)\n",
    "### ![@gyleodhis](./data/gyle.jpg)\n",
    "#### Licence:\n",
    "You can use this code for anything you may wish only leave this page:\n",
    "#### AS IS; HOW IS, WHERE IS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "spark=SparkSession.builder.appName('linear Regression').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1232, 6)\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"./data/Linear_regression_dataset.csv\", inferSchema=True, header=True)\n",
    "print((df.count(), len(df.columns))) # prints number of rows by number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- var_1: integer (nullable = true)\n",
      " |-- var_2: integer (nullable = true)\n",
      " |-- var_3: integer (nullable = true)\n",
      " |-- var_4: double (nullable = true)\n",
      " |-- var_5: double (nullable = true)\n",
      " |-- output: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() # prints the datatype of each column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "|summary|            var_1|            var_2|             var_3|               var_4|               var_5|             output|\n",
      "+-------+-----------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "|  count|             1232|             1232|              1232|                1232|                1232|               1232|\n",
      "|   mean|715.0819805194806|715.0819805194806| 80.90422077922078|  0.3263311688311693| 0.25927272727272715|0.39734172077922014|\n",
      "| stddev| 91.5342940441652|93.07993263118064|11.458139049993724|0.015012772334166148|0.012907228928000298|0.03326689862173776|\n",
      "|    min|              463|              472|                40|               0.277|               0.214|              0.301|\n",
      "|    max|             1009|             1103|               116|               0.373|               0.294|              0.491|\n",
      "+-------+-----------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show() # statistical measures of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|corr(var_1, output)|\n",
      "+-------------------+\n",
      "| 0.9187399607627283|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can check the correlation between the input variables and the output variable  using the corr function:\n",
    "from pyspark.sql.functions import corr\n",
    "df.select(corr('var_1','output')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "var_1 seems to be most strongly correlated with the output column as value close to one indicate strong correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "We will now create a single vector combining all input features by using Spark's VectorAssembler.\n",
    "It merges all input columns into a single feature vector column. You have the freedom to select the number of columns to be used as input columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- var_1: integer (nullable = true)\n",
      " |-- var_2: integer (nullable = true)\n",
      " |-- var_3: integer (nullable = true)\n",
      " |-- var_4: double (nullable = true)\n",
      " |-- var_5: double (nullable = true)\n",
      " |-- output: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "df.columns # returns the number of columns in a dataset\n",
    "vec_assemblers = VectorAssembler(inputCols=['var_1','var_2','var_3','var_4','var_5'], outputCol='features')\n",
    "features_df = vec_assemblers.transform(df)\n",
    "features_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|features                      |\n",
      "+------------------------------+\n",
      "|[734.0,688.0,81.0,0.328,0.259]|\n",
      "|[700.0,600.0,94.0,0.32,0.247] |\n",
      "|[712.0,705.0,93.0,0.311,0.247]|\n",
      "|[734.0,806.0,69.0,0.315,0.26] |\n",
      "|[613.0,759.0,61.0,0.302,0.24] |\n",
      "+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let us return the features columns and see the contents\n",
    "features_df.select('features').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now build a linear regressing between the features column and the output column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------+\n",
      "|features                      |output|\n",
      "+------------------------------+------+\n",
      "|[734.0,688.0,81.0,0.328,0.259]|0.418 |\n",
      "|[700.0,600.0,94.0,0.32,0.247] |0.389 |\n",
      "|[712.0,705.0,93.0,0.311,0.247]|0.417 |\n",
      "|[734.0,806.0,69.0,0.315,0.26] |0.415 |\n",
      "|[613.0,759.0,61.0,0.302,0.24] |0.378 |\n",
      "+------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_df = features_df.select('features','output') # we create a data frame of two colummns\n",
    "model_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the Dataset\n",
    "We have to split the dataset into a training and test dataset in order to train and evaluate the performance of the Linear Regression model built.\n",
    "For this project we chose the 7/3 ratio and train our dataset on 705 of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in Training set: 872\n",
      "Number of records in Testing set: 360\n"
     ]
    }
   ],
   "source": [
    "train_df,test_df = model_df.randomSplit([0.7,0.3])\n",
    "print(\"Number of records in Training set:\",train_df.count())\n",
    "print(\"Number of records in Testing set:\",test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Training Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00034386754040692237,5.740884962235274e-05,0.00020503122467394343,-0.6825195787720568,0.46120112233768223]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lin_Reg = LinearRegression(labelCol='output')\n",
    "lr_model=lin_Reg.fit(train_df)\n",
    "print(lr_model.coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19703658671291857\n",
      "\n",
      "0.872149823810425\n"
     ]
    }
   ],
   "source": [
    "print(lr_model.intercept)\n",
    "print()\n",
    "training_predictions=lr_model.evaluate(train_df)\n",
    "print(training_predictions.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Linear Regression Model on Test Data\n",
    "Here we check the performance of the model on unseen or test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8615781846871451\n",
      "\n",
      "0.00015295779347061328\n"
     ]
    }
   ],
   "source": [
    "test_predictions = lr_model.evaluate(test_df)\n",
    "print(test_predictions.r2)\n",
    "print()\n",
    "print(test_predictions.meanSquaredError)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
