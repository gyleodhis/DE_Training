{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In this project I cover some of the most basic techniques to tackle text data using pyspark.__\n",
    "It good to not that the five major steps for most NLPs are:\n",
    "- Reading the Corps\n",
    "- Tokenization.\n",
    "- Cleaning/ Stopword Removal\n",
    "- Stemming\n",
    "- Converting into numerical form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName('Natural Language Processing').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how we can do tokenization using PySpark. The first step is to create a dataframe that has text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------+\n",
      "|user_id|Review                                    |\n",
      "+-------+------------------------------------------+\n",
      "|1      |I really liked this movie                 |\n",
      "|2      |I would recommend this movie to my friends|\n",
      "|3      |movie was alright but acting was horrible |\n",
      "|4      |I am never watching that movie ever again |\n",
      "+-------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,'I really liked this movie'),\n",
    "                            (2,'I would recommend this movie to my friends'),\n",
    "                            (3,'movie was alright but acting was horrible'),\n",
    "                            (4,'I am never watching that movie ever again')],\n",
    "                           ['user_id','Review'])\n",
    "df.show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to import Tokenizer from the Spark library. We have to then pass the input column and name the output column after tokenization. We use the transform function in order to apply tokenization to the review column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|user_id|              Review|              tokens|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|I really liked th...|[i, really, liked...|\n",
      "|      2|I would recommend...|[i, would, recomm...|\n",
      "|      3|movie was alright...|[movie, was, alri...|\n",
      "|      4|I am never watchi...|[i, am, never, wa...|\n",
      "+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenization = Tokenizer(inputCol='Review', outputCol='tokens')\n",
    "tokenized_df=tokenization.transform(df)\n",
    "tokenized_df.show(10, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The new column contains tokens for each sentence__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words Removal\n",
    "Stop words and little to no value to our analysis. Including them in our computation only increases computation overhead without adding too much insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "|user_id|tokens                                             |refined_tokens                    |\n",
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "|1      |[i, really, liked, this, movie]                    |[really, liked, movie]            |\n",
      "|2      |[i, would, recommend, this, movie, to, my, friends]|[recommend, movie, friends]       |\n",
      "|3      |[movie, was, alright, but, acting, was, horrible]  |[movie, alright, acting, horrible]|\n",
      "|4      |[i, am, never, watching, that, movie, ever, again] |[never, watching, movie, ever]    |\n",
      "+-------+---------------------------------------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stopword_removal=StopWordsRemover(inputCol='tokens', outputCol='refined_tokens')\n",
    "refined_df = stopword_removal.transform(tokenized_df)\n",
    "refined_df.select('user_id', 'tokens', 'refined_tokens').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The refined_tokens column has all the stopwords removed.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (BOW)\n",
    "This is the methodology through which we can represent the text data into numerical form for it to be used by Machine Learning or any other analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer\n",
    "This takes the count of the word appearing in aparticular document. Let's see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------+---------------------------------+\n",
      "|user_id|refined_tokens                    |features                         |\n",
      "+-------+----------------------------------+---------------------------------+\n",
      "|1      |[really, liked, movie]            |(11,[0,6,9],[1.0,1.0,1.0])       |\n",
      "|2      |[recommend, movie, friends]       |(11,[0,2,4],[1.0,1.0,1.0])       |\n",
      "|3      |[movie, alright, acting, horrible]|(11,[0,1,7,10],[1.0,1.0,1.0,1.0])|\n",
      "|4      |[never, watching, movie, ever]    |(11,[0,3,5,8],[1.0,1.0,1.0,1.0]) |\n",
      "+-------+----------------------------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "count_vec = CountVectorizer(inputCol='refined_tokens', outputCol='features')\n",
    "cv_df = count_vec.fit(refined_df).transform(refined_df)\n",
    "cv_df.select('user_id','refined_tokens', 'features').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, each sentence is represented as a dense vector. It shows that the vector length is 12 and the first sentence contains 3 values at the 0th, 4th, and 5th indexes.\n",
    "__To validate the vocabulary of the count vectorizer, we can simply use\n",
    "the vocabulary function:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie',\n",
       " 'ever',\n",
       " 'recommend',\n",
       " 'never',\n",
       " 'watching',\n",
       " 'alright',\n",
       " 'horrible',\n",
       " 'acting',\n",
       " 'really',\n",
       " 'friends',\n",
       " 'liked']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.fit(refined_df).vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drawback of using the Count Vectorizer method is that it doesn’t consider the co-occurrences of words in other documents. In simple terms, the words appearing more often would have a larger impact on the feature vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency – inverse Document Frequency (TF-IDF).\n",
    "This method tries to normalize the frequency of word occurrence based on other documents. The whole idea is to give more weight to the word if appearing a high number of times in the same document but penalize if it is appearing a higher number of times in other documents as well. This indicates that a word is common across the corpus and is not as important as its frequency in the current document indicates.\n",
    "- __Term Frequency:__ Score based on the frequency of word in current document.\n",
    "- __Inverse Document Frequency:__ Scoring based on frequency of documents that contains the current word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "|user_id|              Review|              tokens|      refined_tokens|         tf_features|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      1|I really liked th...|[i, really, liked...|[really, liked, m...|(262144,[99172,21...|\n",
      "|      2|I would recommend...|[i, would, recomm...|[recommend, movie...|(262144,[68228,13...|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF,IDF\n",
    "hashing_vec = HashingTF(inputCol='refined_tokens', outputCol='tf_features')\n",
    "hashing_df = hashing_vec.transform(refined_df)\n",
    "hashing_df.show(2,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|user_id|              Review|              tokens|      refined_tokens|         tf_features|     tf_idf_features|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      1|I really liked th...|[i, really, liked...|[really, liked, m...|(262144,[99172,21...|(262144,[99172,21...|\n",
      "|      2|I would recommend...|[i, would, recomm...|[recommend, movie...|(262144,[68228,13...|(262144,[68228,13...|\n",
      "|      3|movie was alright...|[movie, was, alri...|[movie, alright, ...|(262144,[95685,17...|(262144,[95685,17...|\n",
      "|      4|I am never watchi...|[i, am, never, wa...|[never, watching,...|(262144,[63139,11...|(262144,[63139,11...|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vec = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
    "tf_idf_df=tf_idf_vec.fit(hashing_df).transform(hashing_df)\n",
    "tf_idf_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Sentiment Analysis on refined tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
